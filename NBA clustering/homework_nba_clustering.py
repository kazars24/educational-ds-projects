# -*- coding: utf-8 -*-
"""homework_nba_clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e8YCIwGhY6fyucenw9qUF_7HB-_mhzf3

## Данные об игроках NBA
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

# %matplotlib inline
import matplotlib.pyplot as plt

import seaborn as sns

# %matplotlib inline
from matplotlib import pyplot as plt
plt.style.use(['seaborn-darkgrid'])
plt.rcParams['figure.figsize'] = (12, 9)
plt.rcParams['font.family'] = 'DejaVu Sans'

from sklearn import metrics
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import *
from sklearn.cluster import DBSCAN

import matplotlib.cm as cm
from sklearn.manifold import TSNE

nba = pd.read_csv("nba_2013.csv")
nba.head()

#оставляем только числовые признаки
nba = nba[['age','g', 'gs', 'mp', 'fg', 'fga',
       'fg.', 'x3p', 'x3pa', 'x3p.', 'x2p', 'x2pa', 'x2p.', 'efg.', 'ft',
       'fta', 'ft.', 'orb', 'drb', 'trb', 'ast', 'stl', 'blk', 'tov', 'pf','pts']]

nba.describe()

"""#### Расшифровка некоторых колонок:

player — name of the player

pos — the position of the player

g — number of games the player was in

gs — number of games the player started

pts — total points the player scored

ast - the total number of assists the player had in the season.

fg. - the player's field goal percentage for the season.

подробнее об остальных https://www.basketball-reference.com/about/glossary.html

## Задание


1) Замените пропуски во всех колонках на медиану с помощью SimpleImputer  (Без этих преобразований алгоритмы кластеризации не будут работать из-за пропусков!)
"""

nba.isnull().sum(axis=0)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy ='median')
imputer = imputer.fit(nba)
nba = imputer.transform(nba)

nba

"""2) Стандартизируйте все признаки с помощью MinMaxScaler"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
nba = scaler.fit_transform(nba)

nba

"""Будем работать с преобразованным датасетом.

### Попробуйте кластеризовать данные при помощи KMeans

1. Постройте график инерции (кол-во кластеров от 1 до 10)

Определите по методу локтя оптимальное число кластеров
"""

inertia = []
k = range(1, 10)
for k_i in k:
    kmeans=KMeans(n_clusters=k_i,n_init=100,random_state=123)
    kmeans.fit(nba)
    inertia.append(kmeans.inertia_)
    
plt.plot(k, inertia)
plt.xlabel('k')
plt.ylabel('inercia')

"""2. Постройте график силуета (silhoette_score) (кол-во кластеров от 1 до 10)

"""

from sklearn.metrics import silhouette_score
silhouette = []
k = range(2, 10)
for k_i in k:
    kmeans=KMeans(n_clusters=k_i,n_init=100,random_state=123)
    kmeans.fit(nba)
    silhouette.append(silhouette_score(nba, kmeans.labels_))
print(silhouette)
plt.plot(k, silhouette)
plt.xlabel('k')
plt.ylabel('silhouette')

"""3. Постройте график силуета для каждого кластера отдельно, для этого воспользуйтесь функцией clusters_stats

Пример вызова clusters_stats:
"""

def clusters_stats(n_clusters, clusters, X):
    fig, ax1 = plt.subplots(1, 1)
    ax1.set_xlim([-0.1, 1])
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    silhouette_avg = silhouette_score(X, clusters)
    print("For n_clusters =", n_clusters, "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, clusters)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[clusters == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        
        cmap = cm.get_cmap("Spectral")
        color = cmap(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()

n_clusters = 3
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(nba)
clusters = kmeans.predict(nba)
clusters_stats(n_clusters, clusters,nba )

n_clusters = 6
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(nba)
clusters = kmeans.predict(nba)
clusters_stats(n_clusters, clusters,nba )

"""Проанализируйте полученный результат и определите оптимальное число кластеров. Чему оно равно?

4. Уменьшите количество признаков в исходном датасете nba до 2 с помощью TSNE c параметром perplexity=30. Результат запишите в таблицу tsne_representation.
"""

tsne_representation = TSNE(n_components=2, perplexity=30).fit_transform(nba)
tsne_representation

"""5. Теперь обучите Kmeans еще раз задав оптимальное число кластеров. Визуализируйте полученное разбиение на преобразованном датасете tsne_representation (каждый из 2 "сжатых" признаков в таблице tsne_representation - это ось на графике). Пример визуализации:"""

km=KMeans(n_clusters=3,n_init=100,random_state=123)
km.fit(tsne_representation)
cluster_labels=km.labels_

colors = cm.rainbow(np.linspace(0, 1, len(set(km.labels_))))

for y, c in zip(set(km.labels_), colors):
    plt.scatter(pd.DataFrame(tsne_representation)[0].loc[pd.DataFrame(km.labels_).values==y], 
                pd.DataFrame(tsne_representation)[1].loc[pd.DataFrame(km.labels_).values==y], color=c, alpha=0.5, label=str(y))
plt.legend()

"""### Теперь попробуем метод DBScan

1. Визуализируйте параметр eps в цикле для разных min_samples(от 3 до 10)

Выберите оптимальное значение min_samples и eps. Eps определите по методы локтя. Min_samples выбирайте исходя из 2 критериев: чем меньше оптимальное eps - тем лучше, чем больше точек по оси Х остается (не записывается в выбросы) - тем лучше.
"""

from sklearn.neighbors import NearestNeighbors

e = []
min_samples = range(3, 11)
for i in min_samples:
    neigh = NearestNeighbors(n_neighbors=i)
    nbrs = neigh.fit(nba)
    distances, indices = nbrs.kneighbors(nba)
    distances = np.sort(distances, axis=0)
    distances = distances[:,1]
    e.append(max(distances))
plt.plot(min_samples, e)
plt.xlabel('min_samples')
plt.ylabel('e')

"""Проанализируйте полученный результат и определите оптимальное число кластеров. Чему оно равно?

2. Теперь обучите DBScan еще раз, задав оптимальное число кластеров. Визуализируйте полученное разбиение на преобразованном датасете tsne_representation (каждый из 2 "сжатых" признаков в таблице tsne_representation - это ось на графике). Пример визуализации:
"""

db = DBSCAN(eps=0.855, min_samples=5).fit(tsne_representation)

colors = cm.rainbow(np.linspace(0, 1, len(set(km.labels_))))

for y, c in zip(set(db.labels_), colors):
    plt.scatter(pd.DataFrame(tsne_representation)[0].loc[pd.DataFrame(db.labels_).values==y], 
                pd.DataFrame(tsne_representation)[1].loc[pd.DataFrame(db.labels_).values==y], color=c, alpha=0.5, label=str(y))
plt.legend()